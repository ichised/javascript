{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/ichised/javascript/blob/master/LSTM.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "khlQRbqP5grj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5475
        },
        "outputId": "a74f0725-d78e-4434-b2b0-16a969007e23"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import SGD\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "class Predictor(nn.Module):\n",
        "    def __init__(self, inputDim, hiddenDim, outputDim):\n",
        "        super(Predictor, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size = inputDim,\n",
        "                            hidden_size = hiddenDim,\n",
        "                            batch_first = True)\n",
        "        self.output_layer = nn.Linear(hiddenDim, outputDim)\n",
        "    \n",
        "    def forward(self, inputs, hidden0=None):\n",
        "        output, (hidden, cell) = self.rnn(inputs, hidden0)\n",
        "        output = self.output_layer(output[:, -1, :])\n",
        "\n",
        "        return output\n",
        "\n",
        "def mkDataSet(data_size, data_length=50, freq=60., noise=0.00):\n",
        "    \"\"\"\n",
        "    params\\n\n",
        "    data_size : データセットサイズ\\n\n",
        "    data_length : 各データの時系列長\\n\n",
        "    freq : 周波数\\n\n",
        "    noise : ノイズの振幅\\n\n",
        "    returns\\n\n",
        "    train_x : トレーニングデータ（t=1,2,...,size-1の値)\\n\n",
        "    train_t : トレーニングデータのラベル（t=sizeの値）\\n\n",
        "    \"\"\"\n",
        "    train_x = []\n",
        "    train_t = []\n",
        "\n",
        "    for offset in range(data_size):\n",
        "        train_x.append([[math.sin(2 * math.pi * (offset + i) / freq) + np.random.normal(loc=0.0, scale=noise)] for i in range(data_length)])\n",
        "        train_t.append([math.sin(2 * math.pi * (offset + data_length) / freq)])\n",
        "\n",
        "    return train_x, train_t\n",
        "\n",
        "def mkRandomBatch(train_x, train_t, batch_size=10):\n",
        "    \"\"\"\n",
        "    train_x, train_tを受け取ってbatch_x, batch_tを返す。\n",
        "    \"\"\"\n",
        "    batch_x = []\n",
        "    batch_t = []\n",
        "\n",
        "    for _ in range(batch_size):\n",
        "        idx = np.random.randint(0, len(train_x) - 1)\n",
        "        batch_x.append(train_x[idx])\n",
        "        batch_t.append(train_t[idx])\n",
        "    \n",
        "    return torch.tensor(batch_x), torch.tensor(batch_t)\n",
        "\n",
        "def main():\n",
        "    training_size = 10000\n",
        "    test_size = 1000\n",
        "    epochs_num = 1000\n",
        "    hidden_size = 5\n",
        "    batch_size = 100\n",
        "\n",
        "    train_x, train_t = mkDataSet(training_size)\n",
        "    test_x, test_t = mkDataSet(test_size)\n",
        "\n",
        "    model = Predictor(1, hidden_size, 1)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "    for epoch in range(epochs_num):\n",
        "        # training\n",
        "        running_loss = 0.0\n",
        "        training_accuracy = 0.0\n",
        "        for i in range(int(training_size / batch_size)):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            data, label = mkRandomBatch(train_x, train_t, batch_size)\n",
        "\n",
        "            output = model(data)\n",
        "\n",
        "            loss = criterion(output, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.data[0]\n",
        "            training_accuracy += np.sum(np.abs((output.data - label.data).numpy()) < 0.1)\n",
        "\n",
        "        #test\n",
        "        test_accuracy = 0.0\n",
        "        for i in range(int(test_size / batch_size)):\n",
        "            offset = i * batch_size\n",
        "            data, label = torch.tensor(test_x[offset:offset+batch_size]), torch.tensor(test_t[offset:offset+batch_size])\n",
        "            output = model(data, None)\n",
        "\n",
        "            test_accuracy += np.sum(np.abs((output.data - label.data).numpy()) < 0.1)\n",
        "        \n",
        "        training_accuracy /= training_size\n",
        "        test_accuracy /= test_size\n",
        "\n",
        "        print('%d loss: %.3f, training_accuracy: %.5f, test_accuracy: %.5f' % (\n",
        "            epoch + 1, running_loss, training_accuracy, test_accuracy))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:85: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 loss: 49.270, training_accuracy: 0.06660, test_accuracy: 0.06700\n",
            "2 loss: 38.038, training_accuracy: 0.06540, test_accuracy: 0.08400\n",
            "3 loss: 22.477, training_accuracy: 0.09920, test_accuracy: 0.10200\n",
            "4 loss: 10.530, training_accuracy: 0.11120, test_accuracy: 0.11900\n",
            "5 loss: 6.877, training_accuracy: 0.11540, test_accuracy: 0.13600\n",
            "6 loss: 5.651, training_accuracy: 0.13080, test_accuracy: 0.13500\n",
            "7 loss: 4.881, training_accuracy: 0.13790, test_accuracy: 0.15100\n",
            "8 loss: 4.264, training_accuracy: 0.15670, test_accuracy: 0.16800\n",
            "9 loss: 3.781, training_accuracy: 0.17280, test_accuracy: 0.18400\n",
            "10 loss: 3.420, training_accuracy: 0.17900, test_accuracy: 0.18400\n",
            "11 loss: 3.015, training_accuracy: 0.18400, test_accuracy: 0.16700\n",
            "12 loss: 2.725, training_accuracy: 0.20410, test_accuracy: 0.23400\n",
            "13 loss: 2.421, training_accuracy: 0.25070, test_accuracy: 0.25100\n",
            "14 loss: 2.173, training_accuracy: 0.28430, test_accuracy: 0.30100\n",
            "15 loss: 1.927, training_accuracy: 0.29750, test_accuracy: 0.35200\n",
            "16 loss: 1.661, training_accuracy: 0.35500, test_accuracy: 0.35200\n",
            "17 loss: 1.463, training_accuracy: 0.37820, test_accuracy: 0.40100\n",
            "18 loss: 1.299, training_accuracy: 0.42950, test_accuracy: 0.46500\n",
            "19 loss: 1.132, training_accuracy: 0.52640, test_accuracy: 0.56200\n",
            "20 loss: 0.984, training_accuracy: 0.59960, test_accuracy: 0.62900\n",
            "21 loss: 0.847, training_accuracy: 0.64180, test_accuracy: 0.62800\n",
            "22 loss: 0.756, training_accuracy: 0.65980, test_accuracy: 0.67800\n",
            "23 loss: 0.670, training_accuracy: 0.69700, test_accuracy: 0.72900\n",
            "24 loss: 0.597, training_accuracy: 0.75590, test_accuracy: 0.76200\n",
            "25 loss: 0.546, training_accuracy: 0.77370, test_accuracy: 0.79600\n",
            "26 loss: 0.496, training_accuracy: 0.81080, test_accuracy: 0.81300\n",
            "27 loss: 0.466, training_accuracy: 0.85770, test_accuracy: 0.89800\n",
            "28 loss: 0.438, training_accuracy: 0.91370, test_accuracy: 0.93200\n",
            "29 loss: 0.421, training_accuracy: 0.93440, test_accuracy: 0.93200\n",
            "30 loss: 0.398, training_accuracy: 0.93550, test_accuracy: 0.93200\n",
            "31 loss: 0.390, training_accuracy: 0.94200, test_accuracy: 0.94900\n",
            "32 loss: 0.372, training_accuracy: 0.95130, test_accuracy: 0.94900\n",
            "33 loss: 0.361, training_accuracy: 0.95150, test_accuracy: 0.94900\n",
            "34 loss: 0.355, training_accuracy: 0.97570, test_accuracy: 0.98300\n",
            "35 loss: 0.345, training_accuracy: 0.97490, test_accuracy: 1.00000\n",
            "36 loss: 0.336, training_accuracy: 0.99590, test_accuracy: 1.00000\n",
            "37 loss: 0.328, training_accuracy: 0.99780, test_accuracy: 1.00000\n",
            "38 loss: 0.318, training_accuracy: 0.99920, test_accuracy: 1.00000\n",
            "39 loss: 0.314, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "40 loss: 0.309, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "41 loss: 0.306, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "42 loss: 0.296, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "43 loss: 0.293, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "44 loss: 0.285, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "45 loss: 0.280, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "46 loss: 0.270, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "47 loss: 0.272, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "48 loss: 0.265, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "49 loss: 0.265, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "50 loss: 0.256, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "51 loss: 0.256, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "52 loss: 0.251, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "53 loss: 0.247, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "54 loss: 0.240, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "55 loss: 0.240, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "56 loss: 0.235, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "57 loss: 0.232, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "58 loss: 0.225, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "59 loss: 0.228, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "60 loss: 0.221, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "61 loss: 0.216, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "62 loss: 0.216, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "63 loss: 0.212, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "64 loss: 0.205, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "65 loss: 0.205, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "66 loss: 0.204, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "67 loss: 0.201, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "68 loss: 0.196, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "69 loss: 0.196, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "70 loss: 0.195, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "71 loss: 0.192, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "72 loss: 0.188, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "73 loss: 0.184, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "74 loss: 0.183, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "75 loss: 0.179, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "76 loss: 0.178, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "77 loss: 0.174, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "78 loss: 0.175, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "79 loss: 0.173, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "80 loss: 0.171, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "81 loss: 0.169, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "82 loss: 0.164, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "83 loss: 0.162, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "84 loss: 0.162, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "85 loss: 0.160, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "86 loss: 0.159, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "87 loss: 0.159, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "88 loss: 0.154, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "89 loss: 0.154, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "90 loss: 0.151, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "91 loss: 0.148, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "92 loss: 0.149, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "93 loss: 0.148, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "94 loss: 0.146, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "95 loss: 0.144, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "96 loss: 0.144, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "97 loss: 0.140, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "98 loss: 0.140, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "99 loss: 0.139, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "100 loss: 0.136, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "101 loss: 0.137, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "102 loss: 0.134, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "103 loss: 0.134, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "104 loss: 0.132, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "105 loss: 0.130, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "106 loss: 0.131, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "107 loss: 0.127, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "108 loss: 0.127, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "109 loss: 0.126, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "110 loss: 0.122, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "111 loss: 0.123, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "112 loss: 0.123, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "113 loss: 0.122, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "114 loss: 0.121, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "115 loss: 0.119, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "116 loss: 0.117, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "117 loss: 0.118, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "118 loss: 0.117, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "119 loss: 0.115, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "120 loss: 0.114, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "121 loss: 0.114, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "122 loss: 0.112, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "123 loss: 0.111, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "124 loss: 0.109, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "125 loss: 0.109, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "126 loss: 0.108, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "127 loss: 0.106, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "128 loss: 0.106, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "129 loss: 0.105, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "130 loss: 0.105, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "131 loss: 0.104, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "132 loss: 0.103, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "133 loss: 0.102, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "134 loss: 0.101, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "135 loss: 0.102, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "136 loss: 0.098, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "137 loss: 0.099, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "138 loss: 0.100, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "139 loss: 0.099, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "140 loss: 0.098, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "141 loss: 0.096, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "142 loss: 0.095, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "143 loss: 0.095, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "144 loss: 0.095, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "145 loss: 0.093, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "146 loss: 0.094, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "147 loss: 0.093, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "148 loss: 0.091, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "149 loss: 0.090, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "150 loss: 0.090, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "151 loss: 0.089, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "152 loss: 0.087, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "153 loss: 0.087, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "154 loss: 0.087, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "155 loss: 0.086, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "156 loss: 0.087, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "157 loss: 0.086, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "158 loss: 0.085, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "159 loss: 0.084, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "160 loss: 0.084, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "161 loss: 0.084, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "162 loss: 0.084, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "163 loss: 0.083, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "164 loss: 0.081, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "165 loss: 0.082, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "166 loss: 0.081, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "167 loss: 0.080, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "168 loss: 0.078, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "169 loss: 0.079, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "170 loss: 0.077, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "171 loss: 0.078, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "172 loss: 0.078, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "173 loss: 0.078, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "174 loss: 0.076, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "175 loss: 0.077, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "176 loss: 0.075, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "177 loss: 0.075, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "178 loss: 0.075, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "179 loss: 0.075, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "180 loss: 0.073, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "181 loss: 0.073, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "182 loss: 0.074, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "183 loss: 0.072, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "184 loss: 0.071, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "185 loss: 0.071, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "186 loss: 0.072, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "187 loss: 0.071, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "188 loss: 0.070, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "189 loss: 0.070, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "190 loss: 0.069, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "191 loss: 0.070, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "192 loss: 0.068, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "193 loss: 0.069, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "194 loss: 0.068, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "195 loss: 0.068, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "196 loss: 0.067, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "197 loss: 0.067, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "198 loss: 0.066, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "199 loss: 0.067, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "200 loss: 0.065, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "201 loss: 0.066, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "202 loss: 0.064, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "203 loss: 0.064, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "204 loss: 0.064, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "205 loss: 0.063, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "206 loss: 0.062, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "207 loss: 0.064, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "208 loss: 0.063, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "209 loss: 0.062, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "210 loss: 0.062, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "211 loss: 0.062, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "212 loss: 0.062, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "213 loss: 0.061, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "214 loss: 0.061, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "215 loss: 0.061, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "216 loss: 0.060, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "217 loss: 0.060, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "218 loss: 0.060, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "219 loss: 0.059, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "220 loss: 0.059, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "221 loss: 0.058, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "222 loss: 0.058, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "223 loss: 0.060, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "224 loss: 0.058, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "225 loss: 0.058, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "226 loss: 0.057, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "227 loss: 0.058, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "228 loss: 0.057, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "229 loss: 0.056, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "230 loss: 0.056, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "231 loss: 0.057, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "232 loss: 0.056, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "233 loss: 0.055, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "234 loss: 0.055, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "235 loss: 0.055, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "236 loss: 0.054, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "237 loss: 0.055, training_accuracy: 1.00000, test_accuracy: 1.00000\n",
            "238 loss: 0.055, training_accuracy: 1.00000, test_accuracy: 1.00000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b40e0f4e6165>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-b40e0f4e6165>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmkRandomBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b40e0f4e6165>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, hidden0)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvariable_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0;31m# hack to handle LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mLSTMCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0moutgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutgate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mcy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforgetgate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mingate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcellgate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mhy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutgate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "JmHtxCk2Dor0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "90388e99-01ad-44e5-f7af-0971f7848554"
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x58164000 @  0x7f5d82e192a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}